munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/../pers/matei/repos/MultiModalGenAI$ git config  user.email "matei.p.iordanescu@gmail.com"
munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/../pers/matei/repos/MultiModalGenAI$ git config  user.name "matei iordanescu"

munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/../pers/matei/repos$ /usr/local/cuda/bin/nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Wed_Aug_14_10:10:22_PDT_2024
Cuda compilation tools, release 12.6, V12.6.68
Build cuda_12.6.r12.6/compiler.34714021_0

munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/../pers/matei/repos$ cat /usr/local/cuda/version.json

munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers//pers/matei/repos/MultiModalGenAI/docker/v2$ docker run -it --rm --gpus all ubuntu  /bin/bash -c "nvidia-smi"
Tue Dec 24 17:29:38 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.51.01              Driver Version: 565.90         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1660 ...    On  |   00000000:02:00.0 Off |                  N/A |
|  0%   39C    P8              2W /   49W |       0MiB /   6144MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A       110      C   /python3.11                                 N/A      |
+-----------------------------------------------------------------------------------------+

munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/[..]/pers/matei/repos$ dpkg --print-architecture
amd64
__________________________________________________________________________________________

https://www.python.org/downloads/
Python version Maintenance status First released End of support Release schedule
3.13 bugfix 2024-10-07 2029-10 PEP 719
3.12 bugfix 2023-10-02 2028-10 PEP 693

C:\crtUsers\..\pers\matei\repos>git clone https://github.com/Mateiio/MultiModalGenAI.git
Cloning into 'MultiModalGenAI'...
remote: Enumerating objects: 5, done.
remote: Counting objects: 100% (5/5), done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)
Receiving objects: 100% (5/5), done.


https://github.com/microsoft/Phi-3CookBook
https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/
https://medium.com/@researchgraph/what-is-phi-3-5-vision-101ce7c4d410
https://huggingface.co/microsoft/Phi-3.5-vision-instruct

#https://github.com/pytorch/pytorch/blob/main/Dockerfile
https://hub.docker.com/r/huggingface/transformers-pytorch-gpu/dockerfile
https://hub.docker.com/r/huggingface/transformers-pytorch-gpu/




https://discuss.huggingface.co/t/need-example-docker-file-with-gpu-support/57747/3
https://huggingface.co/spaces/SpacesExamples/llama-cpp-python-cuda-gradio/blob/main/Dockerfile
https://github.com/ollama/ollama/blob/main/Dockerfile

https://huggingface.co/spaces/SpacesExamples/Gradio-Docker-Template-nvidia-cuda/blob/main/Dockerfile
https://hub.docker.com/r/nvidia/cuda/tags
#FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04
FROM nvidia/cuda:12.6.1-cudnn-devel-ubuntu22.04


munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/[..]/pers/matei/repos/MultiModalGenAI/docker$ cd /mnt/c/crtUsers/[..]/pers/matei/repos/MultiModalGenAI/docker
munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/[..]/pers/matei/repos/MultiModalGenAI/docker$ docker build -t mmgenai . --no-cache
cd /mnt/c/crtUsers/[...]/pers/matei/repos/MultiModalGenAI/docker
docker build -t mmgenai  -f Dockerfile.MMGenAI . 

cd ./../../
munix@DESKTOP-I37HAO8:/mnt/d/crtusers/Matei/projects$
docker run \
-it \
--rm \
--gpus all \
--name mmgenaicntnr \
-p 8890:8888 \
-v $(pwd):/workspace:rw \
mmgenai \
/bin/bash -c "jupyter lab --notebook-dir=/ --ip='*' --port=8888 --no-browser --allow-root"

http://127.0.0.1:8890/lab

munix@DESKTOP-I37HAO8:~$ cd /mnt/d/crtusers/Matei/projects/
munix@DESKTOP-I37HAO8:/mnt/d/crtusers/Matei/projects$
docker run \
-it \
--rm \
--gpus all \
--name tfgpu \
-p 8891:8888 \
-v $(pwd):/workspace:rw \
tensorflow/tensorflow:2.18.0-gpu-jupyter \
/bin/bash -c "python3 -m pip install --no-cache-dir tensorflow-hub pydot ; jupyter lab --notebook-dir=/ --ip='*' --port=8888 --no-browser --allow-root"

docker run \
-it \
--rm \
--gpus all \
--name tfgpu \
-p 8891:8888 \
-v $(pwd):/workspace:rw \
tensorflow/tensorflow:2.18.0-gpu-jupyter \
/bin/bash -c jupyter lab --notebook-dir=/ --ip='*' --port=8888 --no-browser --allow-root"


munix@DESKTOP-FQLOSI3:/mnt/d/crtusers/Matei/repos/MultiModalGenAI/docker$ bash getDockerFiles.sh
docker build -t mmgenai  -f MMGenAIgpu.Dockerfile .
cd ./docker/v2/
docker build -t mmgenaiv2  -f MMGenAIgpu.Dockerfilev2 .
docker build -t mmgenaiv2  -f HFGPU.Dockerfile .



wget https://raw.githubusercontent.com/huggingface/transformers/refs/heads/main/docker/transformers-all-latest-gpu/Dockerfile  --output-document=HFAllGPU.Dockerfile
docker build -t hfallgpu  -f HFAllGPU.Dockerfile . --no-cache
docker build -t hfallextgpu  -f HFAllGPUext.Dockerfile .  --no-cache
cd ./../../../
docker run \
-it \
--rm \
--gpus all \
--name hfallextgpucntnr \
-p 8890:8888 \
-v $(pwd):/workspace:rw \
hfallextgpu \
/bin/bash -c "jupyter lab --notebook-dir=/ --ip='*' --port=8888 --no-browser --allow-root"

docker build -t hfallext2gpu  -f HFAllGPUext2.Dockerfile .  --no-cache



C:\Windows\System32>taskkill /f /im wslservice.exe
SUCCESS: The process "wslservice.exe" with PID 7588 has been terminated.

PS C:\WINDOWS\system32> wsl --list --verbose
  NAME      STATE           VERSION
* Ubuntu    Stopped         2

wsl.exe --system -d Ubuntu df -h /mnt/wslg/distro



docker rmi $(docker images -a -q)
docker stop $(docker ps -a -q)
docker rm $(docker ps -a -q)
munix@DESKTOP-FQLOSI3:~$ docker image ls
munix@DESKTOP-FQLOSI3:~$ docker container ls
 

https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html



docker run --gpus=all -it --rm --name pytorch -v $(pwd):/workspace:rw -p 8891:8888 quay.io/jupyter/pytorch-notebook:cuda12-python-3.11.8 /bin/bash -c "import torch; print(torch.cuda.is_available())"
docker run --gpus=all -it --rm --name pytorch -v $(pwd):/workspace:rw -p 8891:8888 quay.io/jupyter/pytorch-notebook:cuda12-python-3.11.8 python3 -c "import torch; print(([(i, torch.cuda.get_device_properties(i)) for i in range(torch.cuda.device_count())]))"


docker build -t tfgpu  -f tfgpu.Dockerfile . 
munix@DESKTOP-FQLOSI3:~$ docker run --gpus=all -it --rm --name tfnightly -v $(pwd):/workspace:rw -p 8891:8888 tfgpu python3 -c "from tensorflow.python.client import device_lib; print(device_lib.list_local_devices())"


docker build -t tfgpu  -f tfgpu.Dockerfile .
cd ./../../
docker run \
-it \
--rm \
--gpus all \
--name tfgpucntnr \
-p 8890:8888 \
-v $(pwd):/workspace:rw \
tfgpu \
/bin/bash -c "jupyter lab --notebook-dir=/ --ip='*' --port=8888 --no-browser --allow-root"

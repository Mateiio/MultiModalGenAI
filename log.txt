munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/../pers/matei/repos/MultiModalGenAI$ git config  user.email "matei.p.iordanescu@gmail.com"
munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/../pers/matei/repos/MultiModalGenAI$ git config  user.name "matei iordanescu"

munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/../pers/matei/repos$ /usr/local/cuda/bin/nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Wed_Aug_14_10:10:22_PDT_2024
Cuda compilation tools, release 12.6, V12.6.68
Build cuda_12.6.r12.6/compiler.34714021_0

munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/../pers/matei/repos$ cat /usr/local/cuda/version.json

munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/George/pers/matei/repos/MultiModalGenAI/docker$ docker run -it --rm --gpus all ubuntu  /bin/bash
root@7f7e35d55286:/# nvidia-smi
Sun Oct 20 00:19:42 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 565.51.01              Driver Version: 565.90         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1660 ...    On  |   00000000:02:00.0 Off |                  N/A |
|  0%   39C    P8              6W /   65W |       0MiB /   6144MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/[..]/pers/matei/repos$ dpkg --print-architecture
amd64
__________________________________________________________________________________________

https://www.python.org/downloads/
Python version Maintenance status First released End of support Release schedule
3.13 bugfix 2024-10-07 2029-10 PEP 719
3.12 bugfix 2023-10-02 2028-10 PEP 693

C:\crtUsers\..\pers\matei\repos>git clone https://github.com/Mateiio/MultiModalGenAI.git
Cloning into 'MultiModalGenAI'...
remote: Enumerating objects: 5, done.
remote: Counting objects: 100% (5/5), done.
remote: Compressing objects: 100% (5/5), done.
remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)
Receiving objects: 100% (5/5), done.


https://github.com/microsoft/Phi-3CookBook
https://azure.microsoft.com/en-us/blog/new-models-added-to-the-phi-3-family-available-on-microsoft-azure/
https://medium.com/@researchgraph/what-is-phi-3-5-vision-101ce7c4d410
https://huggingface.co/microsoft/Phi-3.5-vision-instruct

#https://github.com/pytorch/pytorch/blob/main/Dockerfile
https://hub.docker.com/r/huggingface/transformers-pytorch-gpu/dockerfile
https://hub.docker.com/r/huggingface/transformers-pytorch-gpu/




https://discuss.huggingface.co/t/need-example-docker-file-with-gpu-support/57747/3
https://huggingface.co/spaces/SpacesExamples/llama-cpp-python-cuda-gradio/blob/main/Dockerfile
https://github.com/ollama/ollama/blob/main/Dockerfile

https://huggingface.co/spaces/SpacesExamples/Gradio-Docker-Template-nvidia-cuda/blob/main/Dockerfile
https://hub.docker.com/r/nvidia/cuda/tags
#FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04
FROM nvidia/cuda:12.6.1-cudnn-devel-ubuntu22.04


munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/[..]/pers/matei/repos/MultiModalGenAI/docker$ cd /mnt/c/crtUsers/[..]/pers/matei/repos/MultiModalGenAI/docker
munix@DESKTOP-FQLOSI3:/mnt/c/crtUsers/[..]/pers/matei/repos/MultiModalGenAI/docker$ docker build -t mmgenai . --no-cache
cd /mnt/c/crtUsers/[...]/pers/matei/repos/MultiModalGenAI/docker
docker build -t mmgenai  -f Dockerfile.MMGenAI . 

cd ./../../
munix@DESKTOP-I37HAO8:/mnt/d/crtusers/Matei/projects$
docker run \
-it \
--rm \
--gpus all \
--name mmgenaicntnr \
-p 8890:8888 \
-v $(pwd):/workspace:rw \
mmgenai \
/bin/bash -c "jupyter lab --notebook-dir=/ --ip='*' --port=8888 --no-browser --allow-root"

http://127.0.0.1:8890/lab

munix@DESKTOP-I37HAO8:~$ cd /mnt/d/crtusers/Matei/projects/
munix@DESKTOP-I37HAO8:/mnt/d/crtusers/Matei/projects$
docker run \
-it \
--rm \
--gpus all \
--name tfgpu \
-p 8891:8888 \
-v $(pwd):/workspace:rw \
tensorflow/tensorflow:2.18.0-gpu-jupyter \
/bin/bash -c "python3 -m pip install --no-cache-dir tensorflow-hub pydot ; jupyter lab --notebook-dir=/ --ip='*' --port=8888 --no-browser --allow-root"

docker run \
-it \
--rm \
--gpus all \
--name tfgpu \
-p 8891:8888 \
-v $(pwd):/workspace:rw \
tensorflow/tensorflow:2.18.0-gpu-jupyter \
/bin/bash -c jupyter lab --notebook-dir=/ --ip='*' --port=8888 --no-browser --allow-root"

